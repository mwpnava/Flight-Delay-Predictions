{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/resampledDF.csv'\n",
    "df = pd.read_csv(path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCHEDULED_DEPATURE_HR</th>\n",
       "      <th>SCHEDULED_ARRIVAL_HR</th>\n",
       "      <th>AIRLINE_CODE</th>\n",
       "      <th>PRESSURE</th>\n",
       "      <th>PRESSURE_DEST</th>\n",
       "      <th>ORIGIN_AIRPORT_CODE</th>\n",
       "      <th>RH_DEST</th>\n",
       "      <th>RH</th>\n",
       "      <th>DESTINATION_AIRPORT_CODE</th>\n",
       "      <th>DEWPT_DEST</th>\n",
       "      <th>DEWPT</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>29.39</td>\n",
       "      <td>30.04</td>\n",
       "      <td>5</td>\n",
       "      <td>46.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>29.39</td>\n",
       "      <td>30.04</td>\n",
       "      <td>5</td>\n",
       "      <td>46.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>29.39</td>\n",
       "      <td>30.50</td>\n",
       "      <td>5</td>\n",
       "      <td>54.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>30.51</td>\n",
       "      <td>29.26</td>\n",
       "      <td>6</td>\n",
       "      <td>77.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>29.92</td>\n",
       "      <td>29.50</td>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SCHEDULED_DEPATURE_HR  SCHEDULED_ARRIVAL_HR  AIRLINE_CODE  PRESSURE  \\\n",
       "0                      0                     5             3     29.39   \n",
       "1                      0                     5             0     29.39   \n",
       "2                      0                     5             3     29.39   \n",
       "3                      0                     6             0     30.51   \n",
       "4                      0                     8             0     29.92   \n",
       "\n",
       "   PRESSURE_DEST  ORIGIN_AIRPORT_CODE  RH_DEST    RH  \\\n",
       "0          30.04                    5     46.0  62.0   \n",
       "1          30.04                    5     46.0  62.0   \n",
       "2          30.50                    5     54.0  62.0   \n",
       "3          29.26                    6     77.0  56.0   \n",
       "4          29.50                    0     64.0  20.0   \n",
       "\n",
       "   DESTINATION_AIRPORT_CODE  DEWPT_DEST  DEWPT  CATEGORY  \n",
       "0                         5        17.0   25.0         0  \n",
       "1                         5        17.0   25.0         0  \n",
       "2                         7        19.0   25.0         0  \n",
       "3                         0        13.0   20.0         0  \n",
       "4                         2        20.0   18.0         0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = 'CATEGORY'\n",
    "features_names = df.columns.drop(target_name)\n",
    "X = df[features_names]\n",
    "y = df[target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time:  171.88824033737183\n",
      "predict_time:  12.05019736289978\n",
      "accuracy:  0.8662186962121989\n",
      "classification_report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.76      0.81    263450\n",
      "           1       0.86      0.93      0.90    438382\n",
      "\n",
      "    accuracy                           0.87    701832\n",
      "   macro avg       0.87      0.84      0.85    701832\n",
      "weighted avg       0.87      0.87      0.86    701832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, #number of trees in the forest\n",
    "                            random_state=42,  #for reproducibility\n",
    "                            bootstrap=True, #bootstrap samples when building trees\n",
    "                            max_depth=None, #which is the same as having no limit\n",
    "                            max_features='sqrt', #max number of features considered for splitting a node\n",
    "                            min_samples_leaf=1, #min number of samples required to be at a leaf node\n",
    "                            min_samples_split=2, #min number of samples required to split an internal node\n",
    "                            criterion='gini', #measure of quality of a split\n",
    "                            class_weight=None, #which is the same as not using weights,\n",
    "                            min_weight_fraction_leaf=0.0, #min weighted fraction of the sum total of weights required to be at a leaf node\n",
    "                            ccp_alpha=0.0, #complexity parameter used for Minimal Cost-Complexity Pruning,\n",
    "                            max_samples=None, #max number of samples drawn from X to train each base estimator\n",
    "                            max_leaf_nodes=None, #grow trees with max_leaf_nodes in best-first fashion\n",
    "                            min_impurity_decrease=0.0, #a node will be split if this split induces a decrease of the impurity greater than or equal to this value\n",
    "                            n_jobs=-1) #number of jobs to run in parallel\n",
    "\n",
    "start = time.time()\n",
    "rf.fit(X_train, y_train)\n",
    "fit_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf.predict(X_test)\n",
    "predict_time = time.time() - start\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cls_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print('fit_time: ', fit_time)\n",
    "print('predict_time: ', predict_time)\n",
    "print('accuracy: ', accuracy)\n",
    "print('classification_report: ', cls_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time:  30.369096517562866\n",
      "predict_time:  0.49315714836120605\n",
      "accuracy:  0.7792776618905949\n",
      "classification_report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70    263450\n",
      "           1       0.82      0.83      0.82    438382\n",
      "\n",
      "    accuracy                           0.78    701832\n",
      "   macro avg       0.76      0.76      0.76    701832\n",
      "weighted avg       0.78      0.78      0.78    701832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42, #for reproducibility\n",
    "                            criterion='gini', #measure of quality of a split\n",
    "                            splitter='best', #strategy used to choose the split at each node\n",
    "                            max_depth=None, #which is the same as having no limit\n",
    "                            min_samples_split=2, #min number of samples required to split an internal node\n",
    "                            min_samples_leaf=1, #min number of samples required to be at a leaf node\n",
    "                            min_weight_fraction_leaf=0.0, #min weighted fraction of the sum total of weights required to be at a leaf node\n",
    "                            max_features=None, #which is the same as not using any features\n",
    "                            max_leaf_nodes=None, #grow trees with max_leaf_nodes in best-first fashion\n",
    "                            min_impurity_decrease=0.0, #a node will be split if this split induces a decrease of the impurity greater than or equal to this value\n",
    "                            ccp_alpha=0.0, #complexity parameter used for Minimal Cost-Complexity Pruning\n",
    "                            class_weight=None) #which is the same as not using weights\n",
    "\n",
    "start = time.time()\n",
    "dt.fit(X_train, y_train)\n",
    "fit_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "y_pred = dt.predict(X_test)\n",
    "predict_time = time.time() - start\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cls_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print('fit_time: ', fit_time)\n",
    "print('predict_time: ', predict_time)\n",
    "print('accuracy: ', accuracy)\n",
    "print('classification_report: ', cls_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time:  10.31286907196045\n",
      "predict_time:  46.872979402542114\n",
      "accuracy:  0.8743816183930058\n",
      "classification_report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.74      0.82    263450\n",
      "           1       0.86      0.95      0.90    438382\n",
      "\n",
      "    accuracy                           0.87    701832\n",
      "   macro avg       0.88      0.85      0.86    701832\n",
      "weighted avg       0.88      0.87      0.87    701832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3, #number of neighbors to use by default for kneighbors queries\n",
    "                            weights='uniform', #weight function used in prediction\n",
    "                            algorithm='auto', #algorithm used to compute the nearest neighbors\n",
    "                            leaf_size=30, #leaf size passed to BallTree or KDTree\n",
    "                            p=2, #power parameter for the Minkowski metric\n",
    "                            metric='minkowski', #the distance metric to use for the tree\n",
    "                            metric_params=None, #additional keyword arguments for the metric function\n",
    "                            n_jobs=-1) #number of parallel jobs to run\n",
    "\n",
    "start = time.time()\n",
    "knn.fit(X_train, y_train)\n",
    "fit_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "y_pred = knn.predict(X_test)\n",
    "predict_time = time.time() - start\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cls_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print('fit_time: ', fit_time)\n",
    "print('predict_time: ', predict_time)\n",
    "print('accuracy: ', accuracy)\n",
    "print('classification_report: ', cls_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time:  17.89913010597229\n",
      "predict_time:  0.047102928161621094\n",
      "accuracy:  0.6525920733166912\n",
      "classification_report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.29      0.38    263450\n",
      "           1       0.67      0.87      0.76    438382\n",
      "\n",
      "    accuracy                           0.65    701832\n",
      "   macro avg       0.62      0.58      0.57    701832\n",
      "weighted avg       0.63      0.65      0.62    701832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', #used to specify the norm used in the penalization\n",
    "                        dual=False, #select the algorithm to either solve the dual or primal optimization problem\n",
    "                        tol=0.0001, #tolerance for stopping criteria\n",
    "                        C=1.0, #inverse of regularization strength\n",
    "                        fit_intercept=True, #whether to calculate the intercept for this model\n",
    "                        intercept_scaling=1, #when self.fit_intercept is set to True, instance vector x becomes [x, self.intercept_scaling],\n",
    "                        class_weight=None, #which is the same as not using weights\n",
    "                        random_state=42, #for reproducibility\n",
    "                        solver='lbfgs', #algorithm to use in the optimization problem\n",
    "                        max_iter=100, #maximum number of iterations taken for the solvers to converge\n",
    "                        multi_class='auto', #if the option chosen is 'ovr', then a binary problem is fit for each label\n",
    "                        verbose=0, #for the liblinear and lbfgs solvers set verbose to any positive number for verbosity\n",
    "                        warm_start=False, #when set to True, reuse the solution of the previous call to fit as initialization,\n",
    "                        n_jobs=-1, #number of CPU cores used when parallelizing over classes if multi_class='ovr'\n",
    "                        l1_ratio=None) #the Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1\n",
    "\n",
    "start = time.time()\n",
    "lr.fit(X_train, y_train)\n",
    "fit_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "y_pred = lr.predict(X_test)\n",
    "predict_time = time.time() - start\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cls_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print('fit_time: ', fit_time)\n",
    "print('predict_time: ', predict_time)\n",
    "print('accuracy: ', accuracy)\n",
    "print('classification_report: ', cls_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time:  1139.9256253242493\n",
      "predict_time:  1.216982364654541\n",
      "accuracy:  0.6849944146177432\n",
      "classification_report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.49      0.54    263450\n",
      "           1       0.72      0.80      0.76    438382\n",
      "\n",
      "    accuracy                           0.68    701832\n",
      "   macro avg       0.66      0.65      0.65    701832\n",
      "weighted avg       0.68      0.68      0.68    701832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NN\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(100, ), #the ith element represents the number of neurons in the ith hidden layer\n",
    "                    activation='relu', #activation function for the hidden layer\n",
    "                    solver='adam', #the solver for weight optimization\n",
    "                    alpha=0.0001, #L2 penalty (regularization term) parameter\n",
    "                    batch_size='auto', #size of minibatches for stochastic optimizers\n",
    "                    learning_rate='constant', #learning rate schedule for weight updates\n",
    "                    learning_rate_init=0.001, #initial learning rate used\n",
    "                    power_t=0.5, #exponent for inverse scaling learning rate\n",
    "                    max_iter=200, #maximum number of iterations\n",
    "                    shuffle=True, #whether to shuffle samples in each iteration\n",
    "                    random_state=42, #for reproducibility\n",
    "                    tol=0.0001, #tolerance for the optimization\n",
    "                    verbose=False, #whether to print progress messages to stdout\n",
    "                    warm_start=False, #when set to True, reuse the solution of the previous call to fit as initialization\n",
    "                    momentum=0.9, #momentum for gradient descent update\n",
    "                    nesterovs_momentum=True, #whether to use Nesterov's momentum\n",
    "                    early_stopping=False, #whether to use early stopping to terminate training when validation score is not improving\n",
    "                    validation_fraction=0.1, #the proportion of training data to set aside as validation set for early stopping\n",
    "                    beta_1=0.9, #exponential decay rate for estimates of first moment vector in adam, should be in [0, 1)\n",
    "                    beta_2=0.999, #exponential decay rate for estimates of second moment vector in adam, should be in [0, 1)\n",
    "                    epsilon=1e-08, #value for numerical stability in adam\n",
    "                    n_iter_no_change=10, #maximum number of epochs to not meet tol improvement\n",
    "                    max_fun=15000) #maximum number of function evaluations\n",
    "\n",
    "start = time.time()\n",
    "nn.fit(X_train, y_train)\n",
    "fit_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "y_pred = nn.predict(X_test)\n",
    "predict_time = time.time() - start\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cls_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print('fit_time: ', fit_time)\n",
    "print('predict_time: ', predict_time)\n",
    "print('accuracy: ', accuracy)\n",
    "print('classification_report: ', cls_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time:  584.1369001865387\n",
      "predict_time:  1.0987548828125\n",
      "accuracy:  0.7140184545589258\n",
      "classification_report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.44      0.54    263450\n",
      "           1       0.72      0.88      0.79    438382\n",
      "\n",
      "    accuracy                           0.71    701832\n",
      "   macro avg       0.70      0.66      0.67    701832\n",
      "weighted avg       0.71      0.71      0.70    701832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#gradient boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(loss='log_loss', #loss function to be optimized\n",
    "                                learning_rate=0.1, #learning rate shrinks the contribution of each tree by learning_rate\n",
    "                                n_estimators=100, #the number of boosting stages to perform\n",
    "                                subsample=1.0, #the fraction of samples to be used for fitting the individual base learners\n",
    "                                criterion='friedman_mse', #the function to measure the quality of a split\n",
    "                                min_samples_split=2, #the minimum number of samples required to split an internal node\n",
    "                                min_samples_leaf=1, #the minimum number of samples required to be at a leaf node\n",
    "                                min_weight_fraction_leaf=0.0, #the minimum weighted fraction of the sum total of weights (of all the input samples)\n",
    "                                max_depth=3, #maximum depth of the individual regression estimators\n",
    "                                min_impurity_decrease=0.0, #a node will be split if this split induces a decrease of the impurity greater than or equal to this value\n",
    "                                init=None, #an estimator object that is used to compute the initial predictions\n",
    "                                random_state=42, #for reproducibility\n",
    "                                max_features=None, #the number of features to consider when looking for the best split\n",
    "                                verbose=0, #enable verbose output\n",
    "                                max_leaf_nodes=None, #grow trees with max_leaf_nodes in best-first fashion\n",
    "                                warm_start=False, #when set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble\n",
    "                                validation_fraction=0.1, #the proportion of training data to set aside as validation set for early stopping\n",
    "                                n_iter_no_change=None, #n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving\n",
    "                                tol=0.0001, #tol is the threshold for measuring the increase of the validation score. If the increase is smaller than tol, the training stops\n",
    "                                ccp_alpha=0.0) #complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen\n",
    "\n",
    "start = time.time()\n",
    "gb.fit(X_train, y_train)\n",
    "fit_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "y_pred = gb.predict(X_test)\n",
    "predict_time = time.time() - start\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cls_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print('fit_time: ', fit_time)\n",
    "print('predict_time: ', predict_time)\n",
    "print('accuracy: ', accuracy)\n",
    "print('classification_report: ', cls_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save each model, as pickle\n",
    "\n",
    "import pickle\n",
    "#ts = time.time(), round\n",
    "ts = round(time.time())\n",
    "\n",
    "with open(f'lr_{ts}.pkl', 'wb') as f:\n",
    "    pickle.dump(knn, f)\n",
    "\n",
    "with open(f'dt_{ts}.pkl', 'wb') as f:\n",
    "    pickle.dump(dt, f)\n",
    "\n",
    "with open(f'rf_{ts}.pkl', 'wb') as f:\n",
    "    pickle.dump(rf, f)\n",
    "\n",
    "with open(f'nn_{ts}.pkl', 'wb') as f:\n",
    "    pickle.dump(nn, f)\n",
    "\n",
    "with open(f'gb_{ts}.pkl', 'wb') as f:\n",
    "    pickle.dump(gb, f)\n",
    "\n",
    "with open(f'knn_{ts}.pkl', 'wb') as f:\n",
    "    pickle.dump(knn, f)\n",
    "\n",
    "import os\n",
    "#print size of models\n",
    "size_lr = os.path.getsize(f'lr_{ts}.pkl')\n",
    "size_dt = os.path.getsize(f'dt_{ts}.pkl')\n",
    "size_rf = os.path.getsize(f'rf_{ts}.pkl')\n",
    "size_nn = os.path.getsize(f'nn_{ts}.pkl')\n",
    "size_gb = os.path.getsize(f'gb_{ts}.pkl')\n",
    "size_knn = os.path.getsize(f'knn_{ts}.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
